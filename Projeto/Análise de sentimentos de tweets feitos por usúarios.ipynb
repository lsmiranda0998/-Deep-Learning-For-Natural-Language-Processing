{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hourly-jonathan",
   "metadata": {},
   "source": [
    "# Análise de sentimentos em tweets feito por usuários do Twitter\n",
    "\n",
    "       Para este projeto, sera proposto uma forma de realizar análise de sentimentos em tweets feitos por usúarios do Twitter (em inglês), seja um tweet positivo, negativo ou neutro. Para isso, utilizaremos a linguagem de programação Python com o auxilio da biblioteca nltk que possui um corpus disponível para uso de 30.000 tweets, sendo dividos entre 5.000 tweets positivos, 5.000 tweets negativos e 20.000 tweets neutros.\n",
    "\n",
    "## Leitura da base de dados\n",
    "\n",
    "        A biblioteca nltk fornece um corpus livre para uso. Para utilizar ele primeiramente é necessário instalar a base de dados e o punkt que realiza a tokenização da mesma, via os comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-criterion",
   "metadata": {},
   "source": [
    "Uma vez com o download concluído, podemos realizar a leitura da base de dados e exibir no console um exemplo de tweet após a tokenização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "damaged-relaxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "neutral_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-singles",
   "metadata": {},
   "source": [
    "## Normalização dos dados\n",
    "\n",
    "    Palavras possuem diferentes formas, por exemplo: o verbo \"write\" em inglés pode aparecer nas formas \"wrote\" e \"writing\". Dependendo da análise, é necessário normalizar estas palavras para que elas não sejam tratadas como diferentes pelo modelo, como será feito neste projeto. Em NLP, normalização dos dados é o processo de transformar uma palavra para sua forma canônica, ou seja, sempre que aparecer \"wrote\" ou \"writing\" ele será transformado para \"write\". Para este projeto, iremos realizar a normalização dos dados através da técnica de lemmatization (Lematização), para isso será necessário determinar o contexto de cada palavra - em outras palavras, qual a tag de cada palavra, utilizando um postagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daily-delight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-investor",
   "metadata": {},
   "source": [
    "Exibindo o mesmo tweet exemplo do que da última vez, podemos ver que para cada palavra (token) uma tag foi associado – por exemplo, o verbo \"being\" foi associada a tag VBG (Verbo no gerúndio ou pretérito presente). Agora, podemos prosseguir com a técnica de lemetização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tight-signature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-accounting",
   "metadata": {},
   "source": [
    "De forma resumida, uma palavra que possui começa com a tag 'NN' vai ser um noun (substantivo) e uma tag que começa com 'VB' vai ser um verbo. Passando a posição de cada palavra para a função de lemetização, ela nos retorna a sentença com as palavras substituidas para sua forma canônica – como no exemplo do primeiro tweet, podemos perceber que o verbo \"being\" se tornou \"be\" e o substantivo \"members\" se tornou \"member\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-being",
   "metadata": {},
   "source": [
    "## Pré processamento - Remoção de ruídos\n",
    "\n",
    "    Como vimos no nosso tweet de exemplo, há diversos caracteres que não acrescentam nenhuma informação relevante - por exemplo: o '_' e o '@' não são interessantes para a análise de sentimentos. Além destes caracteres irrelevantes, hyperlinks e stopwords como (the,a,an, etc)  também são comuns em tweets, e nao irá agregar nada para a análise de sentimentos. Mais ainda, todos as letras serão transformados em minisculas para facilitar ainda mais o entendimento da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "junior-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tweet_tokens_example = lemmatize_sentence(tweet_tokens[0])\n",
    "def pre_processing(tweet_tokens, stop_words = ()):\n",
    "    new_tokens = []\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        if len(token) >= 1 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            new_tokens.append(token.lower())\n",
    "    return new_tokens\n",
    "stop_words = stopwords.words('english')\n",
    "print(pre_processing(tweet_tokens_example, stop_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-dining",
   "metadata": {},
   "source": [
    "Executando a função de pré processamento todas as letras foram transformadas em minusculas e os hyperlinks, stopwords como \"in, the, my\" e os '@' foram removidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-webster",
   "metadata": {},
   "source": [
    "## Treinamento do modelo e avaliação dos resultados\n",
    "\n",
    "    Para o treinamento do modelo em python, primeiro é necessário converter os tweets tokenizados para um dicionário. Para isso, basta usarmos a função de conversão, passando por parametrôs uma lista de tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "exposed-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_dict(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "        \n",
    "positive_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "neutral_tokens = twitter_samples.tokenized('tweets.20150430-223406.json')\n",
    "positive_tokens_list = []\n",
    "negative_tokens_list = []\n",
    "neutral_tokens_list = []\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "for tokens in positive_tokens:\n",
    "    positive_tokens_list.append(pre_processing(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tokens:\n",
    "    negative_tokens_list.append(pre_processing(tokens, stop_words))\n",
    "\n",
    "for tokens in neutral_tokens:\n",
    "    neutral_tokens_list.append(pre_processing(tokens, stop_words))\n",
    "\n",
    "positive_tokens_for_model = convert_list_to_dict(positive_tokens_list)\n",
    "negative_tokens_for_model = convert_list_to_dict(negative_tokens_list)\n",
    "neutral_tokens_for_model = convert_list_to_dict(neutral_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-vision",
   "metadata": {},
   "source": [
    "Executando o trecho de código acima, criamos uma lista para cada tipo de tokens: positivo, negativo e neutro e adicionamos os respectivos tokens pré processados em sua respectiva lista. Após isso, utilizamos a função convert_to_list para converter as listas criadas para um dicionário. A base de dados foi dividida em 80% (24.000) para o treinamento e 20% (6.000) para testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "subsequent-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "neutral_dataset = [(tweet_dict, \"Neutral\")\n",
    "                     for tweet_dict in neutral_tokens_for_model]\n",
    "dataset = positive_dataset + negative_dataset + neutral_dataset\n",
    "random.shuffle(dataset)\n",
    "train_data = dataset[:24000]\n",
    "test_data = dataset[24000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-break",
   "metadata": {},
   "source": [
    "Para este projeto, foi utilizado o classificador Naive Bayes, um classificador probabilístico baseado na aplicação do Teorema de Bayes, que por sua vez possui fortes premisas independentes entre as características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "internal-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-halloween",
   "metadata": {},
   "source": [
    "Com duração de aproximadamente 5 minutos, o modelo foi treinado podemos ver os resultados obtidos além das 10 informações mais relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "unlike-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.993\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2366.9 : 1.0\n",
      "                      :) = True           Positi : Negati =   1867.8 : 1.0\n",
      "                      :d = True           Positi : Neutra =   1359.7 : 1.0\n",
      "                     :-( = True           Negati : Neutra =    609.7 : 1.0\n",
      "                      rt = True           Neutra : Negati =    588.8 : 1.0\n",
      "                   david = True           Neutra : Positi =    221.9 : 1.0\n",
      "                    tory = True           Neutra : Positi =    221.1 : 1.0\n",
      "                     :-) = True           Positi : Neutra =    206.8 : 1.0\n",
      "                   wanna = True           Negati : Neutra =    187.6 : 1.0\n",
      "                      <3 = True           Positi : Neutra =    177.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-bonus",
   "metadata": {},
   "source": [
    "O modelo obteve uma acurácia de 99.2%. Na tabela de informações mais relavantes, cada linha informa a relação sobre o grupo de tweets em que o mesmo mais apareceu pelo grupo em que ele menos apareceu, por exemplo: o token mais comum na base de dados é o emoticon de tristeza e possui uma razão de 5660/1 tweets para o grupo neutro, ou seja, em somente 1 tweet a cada 5660 que possui o emoticon ':(', não estará relacionado a um sentimento negativo. Um ótimo resultado!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-absolute",
   "metadata": {},
   "source": [
    "Podemos ainda realizar testes com tweets personalizados. No exemplo abaixo, hipoteticamente tweetamos um tweet com sentimento de tristeza, ou seja, negativo. Vamos ver como o nosso modelo se comporta e se ele irá acertar o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "metropolitan-bradley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I lost my dog yesterday. Feels bad! :(\"\n",
    "\n",
    "custom_tokens = pre_processing(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-velvet",
   "metadata": {},
   "source": [
    "Traduzido, tweetamos que nosso cachorro morreu ontem o que é um sentimento negativo. Usando esse tweet para entrada do modelo, ele nos retornou a saída corretamente. Agora vamos realizar um novo teste, mas dessa vez com um sentimento alegre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "prerequisite-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I got lucky today and I won 50$ betting with a friend! =D\"\n",
    "\n",
    "custom_tokens = pre_processing(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-worship",
   "metadata": {},
   "source": [
    "Traduzido, tweetamos que ganhamos 50 dolares em um aposta com um amigo e o modelo preveu que era um sentimento positivo. Correto! Por fim, testaremos o modelo para um sentimento neutro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "occupational-spouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I played soccer with David yesterday.\"\n",
    "\n",
    "custom_tokens = pre_processing(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-straight",
   "metadata": {},
   "source": [
    "Traduzindo, tweetamos que jogamos futebol com o David ontem e o modelo prevêu corretamente mais uma vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-avenue",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Foi treinado um modelo capaz de realizar análise de sentimentos em tweets do idioma inglês. O modelo obteve uma boa performance para o problema em questão. Escolhi esse tema por me despertar um interesse depois que eu li um artigo que realizava essa mesma coisa, mas para criticas de filmes. Acredito que foi um tema legal de se trabalhar e eu espero que seja relevante para o projeto da disciplina.\n",
    "Leonardo Santos Miranda - Mestrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-furniture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
